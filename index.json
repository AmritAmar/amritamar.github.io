[{"authors":["admin"],"categories":null,"content":"I am a software engineer who enjoys challenges and problem-solving. I am excited to apply my expertise in AI, Data, and Software Engineering to have a positive impact on the world.\nPreviously, I was a software engineer at Meta Reality Labs Research where I worked on the future of Augmented Reality and Contextualized AI by developing Project Aria. I then spearheaded the data engineering team at Inflexion Games on a live-service survival multiplayer game called Nightingale. I do part-time consulting on AI and Data projects with businesses who want to reach their potential.\nI graduated from Cornell University with a Bachelor\u0026rsquo;s in 2019 and a Master\u0026rsquo;s in 2020. I possess experience in AI, Data, Embedded Systems, Robotics, Augmented/Virtual Reality, Game Design, and Computer Graphics.\nI am also a top scholar achiever from Botswana, a pianist, a scuba diver, an avid competitive gamer, and a dog person (look at Ekko). Feel free to directly message or email me about anything!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://amritamar.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a software engineer who enjoys challenges and problem-solving. I am excited to apply my expertise in AI, Data, and Software Engineering to have a positive impact on the world.\nPreviously, I was a software engineer at Meta Reality Labs Research where I worked on the future of Augmented Reality and Contextualized AI by developing Project Aria. I then spearheaded the data engineering team at Inflexion Games on a live-service survival multiplayer game called Nightingale.","tags":null,"title":"Amrit Amar","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f8324b86e30c02f1db0e5a1f31e88ef1","permalink":"https://amritamar.github.io/archive/slider/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/slider/","section":"archive","summary":"","tags":null,"title":"","type":"archive"},{"authors":null,"categories":null,"content":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9de239fbe1baa13cff72a1dbf2a30827","permalink":"https://amritamar.github.io/archive/hero/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/hero/","section":"archive","summary":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar","tags":null,"title":"Academic","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"77140deb814caf12eab3729ff0b56537","permalink":"https://amritamar.github.io/archive/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/skills/","section":"archive","summary":"","tags":null,"title":"Skills","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14e332d49095350db2bebd3a5300ab80","permalink":"https://amritamar.github.io/archive/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/experience/","section":"archive","summary":"","tags":null,"title":"Experience","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6a15459c49ea526b336c9c2700601cea","permalink":"https://amritamar.github.io/archive/accomplishments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/accomplishments/","section":"archive","summary":"","tags":null,"title":"Accomplish\u0026shy;ments","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a035fa6ecd57a0d5ddf36e0649eb1954","permalink":"https://amritamar.github.io/archive/posts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/posts/","section":"archive","summary":"","tags":null,"title":"Recent Posts","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7655b370a1acda01108ca8bab3874ed8","permalink":"https://amritamar.github.io/archive/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/projects/","section":"archive","summary":"","tags":null,"title":"Projects","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6df377d73328fa8fdb8286e6a847ca8f","permalink":"https://amritamar.github.io/archive/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/people/","section":"archive","summary":"","tags":null,"title":"People","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"01d61ff578b878f0d04711eee5922d8e","permalink":"https://amritamar.github.io/archive/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/talks/","section":"archive","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9f0af46d651200606dc51bec92138d5c","permalink":"https://amritamar.github.io/archive/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/featured/","section":"archive","summary":"","tags":null,"title":"Featured Publications","type":"archive"},{"authors":null,"categories":null,"content":"Welcome to the Academic Kickstart template!\nFollow our Getting Started and Page Builder guides to easily personalize the template and then add your own content.\nFor inspiration, check out the Markdown files which power the personal demo. The easiest way to publish your new site to the internet is with Netlify.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt    This homepage section is an example of adding elements to the Blank widget.\nBackgrounds can be applied to any section. Here, the background option is set give a color gradient.\nTo remove this section, delete content/home/demo.md.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9c5d2036d30ea9de785a143cca16e39a","permalink":"https://amritamar.github.io/archive/demo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/demo/","section":"archive","summary":"Welcome to the Academic Kickstart template!\nFollow our Getting Started and Page Builder guides to easily personalize the template and then add your own content.\nFor inspiration, check out the Markdown files which power the personal demo. The easiest way to publish your new site to the internet is with Netlify.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt    This homepage section is an example of adding elements to the Blank widget.","tags":null,"title":"Academic Kickstart","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bfcd325579a5cfd5b765b644b230b205","permalink":"https://amritamar.github.io/archive/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/tags/","section":"archive","summary":"","tags":null,"title":"Popular Topics","type":"archive"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"abb02d824d4fb5c0fb18ecdae2f6deae","permalink":"https://amritamar.github.io/archive/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/archive/contact/","section":"archive","summary":"","tags":null,"title":"Contact","type":"archive"},{"authors":null,"categories":null,"content":"I only recently learned about Unity\u0026rsquo;s ML Agents Library - an open-source project to enable Unity projects to serve as environments for reinforcement learning algorithms. This is pretty cool because when I first started using Unity for AI, I had to implement the neural network itself and I used genetic algorithms to train my agents. With this project, training autonomous agents has never been easier with provided implementations of common RL algorithms along with a python interface to train! This project was done using PPO.\nThe first step was setting up my environment. I created a new Unity project and then created a virtual environment to get all the python dependencies, which included pytorch and mlagents. Everything worked pretty smoothly - I was able to use my RTX 3080 for training. I then imported the Unity ML Agents module through Unity, it was super simple. After that, I created a basic environment modelled after Unity\u0026rsquo;s own example: Move to Goal.\nIt\u0026rsquo;s a simple setup - an environment with a simple cube agent and a sphere goal surrounded by walls. The agent\u0026rsquo;s motive is simple: move to the goal without touching the wall. I created a script for the agent called \u0026ldquo;MoveToGoalAgent\u0026rdquo; and, to my surprise, it turns out that you don\u0026rsquo;t need to inherit from MonoBehavior, but rather Agents. After reading through the docs, there were 4 key things I had to handle:\n Observation (what does the agent see to make a decision) Decision (the decision itself - handled by the library) Action (how the decision translates to environment actions) Reward (do we reward the agent for the actions it took)  While I thought about several ways to implement the observation, I decided to stick to providing the agent\u0026rsquo;s location and the goal\u0026rsquo;s location, which is a total of 6 inputs. The ml-agents library gives you the option of using continuous or discrete values for outputs. For this project, 2 continuous variables were enough to specify movement in the x or z directions. After testing passing and failing cases, I trained the environment in parallel with 9 separate instances.\nIt\u0026rsquo;s pretty cool to see how they train over time. The red indicates that the agent failed (by hitting a wall) in the last iteration while green indicates the agent got to the goal. After about 2 minutes, I saw greens across the boards. However, ML isn\u0026rsquo;t just about training specific tasks - it\u0026rsquo;s about effectively training behaviours. My agent only knew how to get to the goal in one way. Move the goal elsewhere and it all went downhill.\nIt\u0026rsquo;s interesting to see how the position of the goal is actively affecting the neural network. To solve this, I randomized the starting and ending positions of the goal and the agent. I then re-trained the neural network and after a few minutes, loaded the trained neural network into Unity.\nIt worked really well! There were some boards that had issues during the validation but overall I think it was a success! I am looking forward to using this library for more complex RL tasks.\n","date":1643846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643846400,"objectID":"d0c9165f69f2cb448656c594986ec6bd","permalink":"https://amritamar.github.io/project/unitymlagents_movetogoal/","publishdate":"2022-02-03T00:00:00Z","relpermalink":"/project/unitymlagents_movetogoal/","section":"project","summary":"An exploratory project to take a look at Unity's ML Agent module.","tags":["Personal","AI"],"title":"Exploring Unity ML Agents - Move to Goal","type":"project"},{"authors":null,"categories":null,"content":"Prototypes is a series on mini game prototypes I\u0026rsquo;ve made exploring a particular concept or idea. These games are prototypes - rough, unpolished, and only testing gameplay. By creating these prototypes I hope to gain a better understanding of vaious game design elements and how game designers create memorable, exciting, and fun games.\nTile Punch is a fast-paced \u0026lsquo;card\u0026rsquo; game about studying the opponent and laying out a board that resolves in real-time. You have think fast and provide an answer to your opponent\u0026rsquo;s moves before they execute it. The catch is that you get to see their moves laid out at the beginning of a \u0026lsquo;pattern\u0026rsquo;. At the end of it, the opponent will do something new and now you have to react to a new pattern!\nPlay the game here! (hit the play button on the top left - change the enemy type by going to line 93 and changing the assignment)\nThe game has 3 moves: A (attack 2), D (defend), H (heal 1). Every \u0026lsquo;evaluation phase\u0026rsquo;, an enemy move and a player move is calculated on the board. This evaluation travels right at a fixed interval, consuming player moves. The player can choose 2 randomly generated tiles to put on the board to combat the enemy, planning ahead for future mechanics.\nIn the fight below, I am fighting vs SLIME. Slime\u0026rsquo;s move set is simple, it had 3 A\u0026rsquo;s in a row, and 3 D\u0026rsquo;s in a row. At the end of a pattern, it switches. This means I should have a tile to defend on the triple attacks and use the empty tile slots to attack. Defending against nothing doesn\u0026rsquo;t hurt me so I can use that slot to heal. Alternatively we can attack each other when we both have A\u0026rsquo;s in a column but because SLIME starts at 20 HP vs me at 15 HP, I need to close the gap first!\nOn a technical note: I tried to make the code as friendly to customize as possible. This means enemies have a function that the code calls to decide their next moves based on the previous enemy moves, the player moves, and the move history. This made it fairly easy to add new opponents as ideas came along and allow my friends to add their own enemies to fight against.\nA lot of playtesters liked this one. The frantic decisions to make against a ticking clock is fun and challenging. One main problem playtesters brought up is that because the game moves fast and figuring out the move pattern of the enemy takes time, they often died the first time before they could succeed rather than figuring it out during the fight. One idea to fix this would be to list the previous enemy moves above the enemy to perhaps make pattern recognition easier. One thing the playtesters brought up is that the beginning of a pattern is always the hardest to react to since you don\u0026rsquo;t get enough time to react to the first few columns.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"8d1d5e0cd7719db089ccef895ec984dd","permalink":"https://amritamar.github.io/project/gameprototypes_tilefight/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/project/gameprototypes_tilefight/","section":"project","summary":"A real-time game about planning and outplaying your opponent.","tags":["Personal","Game Design"],"title":"Game Prototypes - Tile Fight","type":"project"},{"authors":null,"categories":null,"content":"The marching squares algorithm is a computer graphics algorithm for creating contours/boundaries based on underlying grid values. It\u0026rsquo;s a really fascinating algorithm with tons of uses in procedural generation. It\u0026rsquo;s also really simple to implement! This implementation uses OpenSimplex Noise.\nNote: the page can be slow when loading these simulations so it\u0026rsquo;s better to look at them individually (and checkout the various settings you can tinker with): (1) non-interpolated, (2) interpolated, (3) metaballs.\n\r I first started with a non-interpolated version of the algorithm to see all the possible boundaries in action. If you go into the editor, you can enable and disable various settings to see the distribution below or the squares. You can also take a snapshot of it and use it in 2D world generation (such as generating cave structures).\n\r The interpolated version makes everything look much smoother and natural. Using the interpolated version and adding shapes instead of noise, we can use the marching squares algorithm to generate Metaballs!\n\r ","date":1629072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629072000,"objectID":"b2317ea7e4160a77b4cea53769c16152","permalink":"https://amritamar.github.io/project/marchingsquares/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/project/marchingsquares/","section":"project","summary":"A look at the marching squares algorithm.","tags":["Personal"],"title":"Marching Squares with p5.js","type":"project"},{"authors":null,"categories":null,"content":"This was a tiny project I made a couple of years ago in an graphics/simulation class to explore steering behaviours from Craig Reynolds\u0026rsquo; Boids. I made it initially in C++ but ported it to p5.js. It\u0026rsquo;s a pretty cool simulation to show emergent behavior from simple rules; super mesmerizing to watch!\n\r Use the sliders to adjust:\n Alignment (how strongly the boids align the to the average direction of other boids in the vicinity)\n Cohesion (how strongly the boids go to the average position of other boids in the vicinity)\n Separation (how strongly the boids avoid each other in the vicinity)\n  ","date":1625529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625529600,"objectID":"3658ee92a7cec7a48c9fc1acafe52f2b","permalink":"https://amritamar.github.io/project/flockingsimulation/","publishdate":"2021-07-06T00:00:00Z","relpermalink":"/project/flockingsimulation/","section":"project","summary":"A mini-project to explore steering behaviors.","tags":["Personal"],"title":"Flocking Simulation using p5.js","type":"project"},{"authors":null,"categories":null,"content":"This was a tiny project to mimic something a project I did in college in a Robotics class. We used inverse kinematics to derive the required matrices and then the forces needed to move a robotic arm. This is a much simpler implementation in 2D. Stuff like this can be used for procedural animations (applying IK to rigging bones) and a ton of other things to create very realistic motion.\n\r ","date":1624579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624579200,"objectID":"c01dd9c233dbc25e2f564fc544983d3a","permalink":"https://amritamar.github.io/project/inversekinematics/","publishdate":"2021-06-25T00:00:00Z","relpermalink":"/project/inversekinematics/","section":"project","summary":"A mini-project to look at simplified inverse kinematics.","tags":["Personal"],"title":"2D Inverse Kinematics","type":"project"},{"authors":null,"categories":null,"content":"Prototypes is a series on mini game prototypes I\u0026rsquo;ve made exploring a particular concept or idea. These games are prototypes - rough, unpolished, and only testing gameplay. By creating these prototypes I hope to gain a better understanding of vaious game design elements and how game designers create memorable, exciting, and fun games.\nPersonally, I think pure turn-based games are a little bit draining to me. I love watching people play them and I love analyzing and understanding the meta behind them but playing them on my own feels a little lack-luster. I think it\u0026rsquo;s because I prefer seeing things resolve simultaneously. I love seeing the action happen because of my decisions - the tense moment of making a choice and waiting for the effect to play out on my screen is exhilirating and fun! This is the main reason behind making \u0026ldquo;Punch!\u0026rdquo;. I wanted to explore the feeling of making quick choices and hoping that you out-smarted your opponent.\nPlay the game here! (hit the play button on the top left!)\nPunch is a multiplayer game! Open the game with a friend (or across 2 browsers), ensure the \u0026lsquo;CLIENT_CODE\u0026rsquo; is the same, along with the realtime parameter set to what you\u0026rsquo;d like to try (true or false)! Then hit the \u0026lsquo;click me\u0026rsquo; button below the main game window. The main game is played by getting your fist to touch the enemy\u0026rsquo;s head. Do this by dragging on your body parts in a particular direction to add a \u0026lsquo;force\u0026rsquo; to it.\n realtime = true: The game is played in real-time with actions happening continuosly. Control the joints of your robot one-by-one and be the first to win!   realtime = false: The game is played in 3 second intervals. You can control the joints of your robot and set each specific \u0026lsquo;force\u0026rsquo; individually! After 3 seconds, the forces resolve and you get to make your next choices!  A little touch on the technical details: I used matter.js as my physics engine and socket to communicate with a glitch server! I send over the positions and velocity of the character based on requests from the server and synchronize them across connected clients. Yes, this means that this game is \u0026lsquo;client-sided\u0026rsquo; and you could cheat by overriding those values. Creating a client-sided game would have been easier but that\u0026rsquo;s a future project for another day.\nFrom playtesting, one of the most interesting things to explore was \u0026lsquo;who had the beatdown\u0026rsquo; (i.e. who had the advantage in a game). There is a steady flow to the game where players have to identify who is the one in the attacking (beat-down) spot and who is the one in the defending (control) spot. As these switched turn after turn, the concept of turns existing in real-time games makes sense - tempo is an important concept in several games. Performance and networking issues aside, playtesters thought it was a really fun game. A meta developed where players would basically protect themselves completely and then send their robot flying at the opponent with the hands in front.\n","date":1621555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621555200,"objectID":"47a1ee3da21db677f9d7e637cd68ead1","permalink":"https://amritamar.github.io/project/gameprototypes_punch/","publishdate":"2021-05-21T00:00:00Z","relpermalink":"/project/gameprototypes_punch/","section":"project","summary":"A multiplayer game about landing a punch before your opponent does.","tags":["Personal","Game Design"],"title":"Game Prototypes - Punch!","type":"project"},{"authors":null,"categories":null,"content":"\r This was a tiny project I made as a primer to explain rendering to a younger cousin of mine! In the sketch above, I cast rays from a randomly moving point and check whether there is a line-line intersection with any of the walls. If there is (and it is the closest), we use that point as an anchor to draw the ray to. This is basic version of how shadows and lighting is done in 2D games.\n\r In the next sketch, I rendered the rays cast out to a simple 3D scene. Each ray create a \u0026lsquo;slice\u0026rsquo; of the view camera (there aren\u0026rsquo;t that many which is why it isn\u0026rsquo;t smooth). The box size and color brightness decreases with distance (using the inverse square law). On the top of the sketch window, you can change the FOV. I thought about making boxes instead of lines but I really like how the lines look with higher FOVs and create some weird illusions.\nThis was a simple look into writing a really quick renderer and simple ray tracer. During college I wrote a ray-tracing engine for an graphics project so doing this bare-bones implementation was pretty interesting.\n","date":1610323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610323200,"objectID":"e92dfafd0ce1837ddab1389ed797cf97","permalink":"https://amritamar.github.io/project/3draycastrendering/","publishdate":"2021-01-11T00:00:00Z","relpermalink":"/project/3draycastrendering/","section":"project","summary":"A fun mini-project to explore the bare-bones of ray tracing and rendering.","tags":["Personal"],"title":"Exploring Simple 3D Rendering using p5.js","type":"project"},{"authors":null,"categories":null,"content":"Prototypes is a series on mini game prototypes I\u0026rsquo;ve made exploring a particular concept or idea. These games are prototypes - rough, unpolished, and only testing gameplay. By creating these prototypes I hope to gain a better understanding of vaious game design elements and how game designers create memorable, exciting, and fun games.\nA game of chance is a card game about calculated risk. I envision this game being played in a dark setting - you vs the world. I\u0026rsquo;ve always been fascinated by repeated games like poker, where your success is measured over numerous plays rather than just a singular encounter. One of my favorite strategy games, Teamfight Tactics, is also like this. I think games with variance have high replayability (see Roguelikes) and create really fun situations that challenge the player to (1) think about what they\u0026rsquo;re dealt and, more importantly, (2) how to make the best use of the situation. While this game isn\u0026rsquo;t that complicated, I wanted to recreate that feeling of high-stakes decision making.\nPlay the game here! (hit the play button on the top left!)\nHow to Play: - In front of you are 9 cards. There are 8 numbered cards (2-9) and 1 death card.\n Every time you opt to turn over a card, you wager 1 gold (this is what the \u0026ldquo;Current\u0026rdquo; means on the UI).\n If you hit a number card, continue! If you hit the death card, you will lose what you wagered + 2 gold.\n If you uncover everything, you win everything you wagered. If you pass the game, you will gain want you wagered - 2 gold.\n In addition, there are power ups to help you! If you get 3 in a row or column, you can use a power up to help you.\n (+) powerup allows you to select any card that hasn\u0026rsquo;t been turned over and you will get a hint as to what the card could be above.\n (\u0026gt;) powerup allows you to select any uncovered card and reveals whether neighbor cards are higher or lower than the chosen card.\n In addition, you can guess where the death card is at any point by hitting the \u0026ldquo;GUESS D\u0026rdquo; button. If you successfully guess, you will gain, in addition to what you wagered, 3 times the number of remaining cards. If you fail, you will lose what you wagered + 10 gold.\n  After having playtests with some of my friends, I got a lot of feedback on the concept. They said that once they got the hang of the game, \u0026lsquo;passing\u0026rsquo; after getting 4-5 cards correct will nearly always net you a win in the long run. They liked that the stakes only increased when you \u0026lsquo;committed\u0026rsquo; more to the game and that losing by getting unlucky didn\u0026rsquo;t feel bad. Some playtesters felt that \u0026lsquo;passing\u0026rsquo; shouldn\u0026rsquo;t even be a part of the game at all. When I tested the game by removing the \u0026lsquo;passing\u0026rsquo;, playtesters felt like they got really unlucky sometimes towards the end which killed their mood if it happened repeatedly. While they liked the powerups, almost all playtesters said that the (+) powerup felt too weak since you could technically get a (2+) on a 7 or 8 which didn\u0026rsquo;t really help them.\nIn the future, a more complete game would probably include a \u0026lsquo;bank\u0026rsquo; of money that you wager (so that there\u0026rsquo;s a concrete lose condition) along with more powerups and perhaps a bigger board (4x3 or 4x4).\n","date":1604448000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604448000,"objectID":"4f775e8c91f8664ece7ad94ea1fdaa39","permalink":"https://amritamar.github.io/project/gameprototypes_gameofchance/","publishdate":"2020-11-04T00:00:00Z","relpermalink":"/project/gameprototypes_gameofchance/","section":"project","summary":"A card game you play when you face death itself.","tags":["Personal","Game Design"],"title":"Game Prototypes - A Game of Chance","type":"project"},{"authors":null,"categories":null,"content":"This is the final project my team and I worked on in advanced artificial intelligence. I give a brief summary of what we did throughout the semester. Unfortunately, I can\u0026rsquo;t release the code as per Cornell regulations, but feel free to read the final paper on the project (linked above), along with the data. All the following pictures were generated by computers!\nCommunication isn’t perfect. Perhaps the room is too noisy or one party is distracted with another task but people often don’t understand each other perfectly; however, communication between people doesn’t break down. Similarly, in writing, we can still make out what the author intended to write on a piece of paper despite stains or wear down. We explore the question, can neural networks encode a language robust enough to be transmitted through noise channels and recreate the original meaning?\nOur goal in this project was to study the robustness of neural networks in encoding a language to communicate with one another. We used neural networks to generate images given vectors and transmitted those images over a noisy channel of communication. Another neural network received these images and decoded the image back to its vector representation.\nOur model is based on the Shannon-Weaver model of communication where the transmitter and receiver roles are played by two neural networks. G encodes a message. D decodes that message. As the networks get better at communicating, we increase the noise in the communication channel.\nWe did the following experiments: - Compare Grayscale and RGB output of networks - Compare the impact of the size of output images between Grayscale models - Compare the impact of the size of output images between RGB models - The impact of noise on a larger dataset on grayscale networks. - Qualitative Evaluation of RGB 64x64 Model with GloVe vectors\nThe first picture in this article belongs to a greyscale model outputting 36 8x8 images (A-Z, 0-9). The picture below is a color model outputting 36 8x8 images.\nExample 64x64 pictures. Greyscale:\nColor:\nWe also did word embeddings using Twitter\u0026rsquo;s GloVe embeddings:\nAn explanation of our experiments and more data can be found in the source can be found on our source page above.\nOverall, qualitatively, our models created images that look distinct from one another. For several of the one-hot models, we can clearly see differences between various encodings, similar to how we see differences between characters of various alphabets. The difference between real life communication and our model is that real life communication is based on alphabets that are defined by outlines and real images, while our models simply try to fill in a box with colors. It would be interesting to extend this project to generate scribbles rather than defined images. Interestingly, the lower the dimension of the generated image, the closer it resembles our own letters. Perhaps with smaller dimensions and more limitations, we could get something that resembles our own alphabet.\n","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590019200,"objectID":"852b1c55423d216ec4088aa8b87036e4","permalink":"https://amritamar.github.io/project/visualizinglanguageovernoise/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/project/visualizinglanguageovernoise/","section":"project","summary":"A GANs Project for CS 6700 (Advanced Artificial Intelligence).","tags":["Cornell","AI"],"title":"Visualizing Language over Noise","type":"project"},{"authors":null,"categories":null,"content":"Games2Anime is a website that recommends Anime based on Steam Games.\nDue to current world events (COVID-19), there is a greater demand for online entertainment. Steam recently reached a record high for concurrent users and more people than ever are consuming online entertainment. However, there are only so many games on Steam and many gamers may seek to broaden their horizons by trying something new, like Japanese animation (anime). However, it can be difficult to narrow what anime to watch due to its wide variety of genres and styles.\nThus, the goal of our application is to recommend anime to people based on their Steam gaming preferences. By giving our system a game title from Steam, it will return a list of recommended anime based on the similarity in reviews and game description. Each recommendation displays general details and a similarity rating, along with a link to its MyAnimeList page.\nIt works by comparing Steam Game Descriptions and Anime User Reviews to give you a list of Anime recommendations.\nGames2Anime gets a list of all Anime with a MAL rating of 7.0 or above. We then remove any sequels/related movies from the list (to ensure we don\u0026rsquo;t recommend more than 1 show from a particular series on a search). We get the top 20 reviews for each anime in our list (sorted by most helpful reviews first). We then combine all the reviews and use a fancy tool called TfidfVectorizer that converts a collection of raw documents to a matrix of TF-IDF features. We then perform dimensionality reduction using singular value decomposition (svds) to get a matrices of words in reviews and anime based on words. We these structures, we can find similar anime to words using cosine similarity.\nWe then get a list of all Steam Games and their descriptions. Based on the game input, we go through each word in the game\u0026rsquo;s description and get the top anime to that word and build up a ranking based on how often an anime shows up across the entire description. After normalizing and sorting the scores, we return the top 5 anime recommendations based on the game.\nBecause we are heavily reliant on keyword matching, some keywords get over weighted and thus we have to manually decrease the weight of certain keywords. Sometimes this affects \u0026lsquo;obvious\u0026rsquo; results like getting basketball anime for a game about basketball. This is ongoing so some search results are not great, yet. In the future, it would be nice to have search result feedback, either using Rocchio or another method. Another improvement could be adding a video API for when we don\u0026rsquo;t find a trailer for an anime.\nThis was a fun final project for CS 4300! I learnt a lot about information retrieval methods and systems along the way and found my next show. I hope it finds your next show as well!\n","date":1588896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588896000,"objectID":"b4f933f40eefc1f224511468aa733f63","permalink":"https://amritamar.github.io/project/games2anime/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/project/games2anime/","section":"project","summary":"An anime recommendation system for CS 4300 (Language and Information).","tags":["Cornell","AI","Game Design"],"title":"Games2Anime","type":"project"},{"authors":null,"categories":null,"content":"\r This was a tiny project I made cause circles are cool! Wikipedia has a great explanation on how to pack circles. The source for circle packing is here and for the image version, here! Feel free to try out different parameters and images in the browser (just change imgSource to point to another picture). Warning - can lag on really big images with different parameters!\n\r The 3 sliders are for counts per update or how many circles are added every frame (1 - 100), total circles or max amount of circles on the screen (0 (= infinity) - 5000), and total attempts or the number of times we try to add a circle before we cut off the algorithm (100 - 2000), respectively. By default, the values are 10, 2500, 1000, but you can vary them!\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"12a3450ea07ccfd4e046cdc30ebc6fbd","permalink":"https://amritamar.github.io/project/circlepacking/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/project/circlepacking/","section":"project","summary":"A fun mini-project because I like circles.","tags":["Personal"],"title":"Exploring Circle Packing using p5.js","type":"project"},{"authors":null,"categories":null,"content":"This is a JS implementation of a BrainFuck interpreter. I used P5JS to create this to understand how interpreters work and, more specifically, how this monstrosity of a programming language works. Mess with the code here!\n\r ","date":1579910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579910400,"objectID":"dd5f3d08b26370a9eaabba4efd71bdac","permalink":"https://amritamar.github.io/project/bfvisualinterpreter/","publishdate":"2020-01-25T00:00:00Z","relpermalink":"/project/bfvisualinterpreter/","section":"project","summary":"An afternoon project to create an interpreter to understand an esoteric language.","tags":["Personal"],"title":"A Visual BF Interpreter","type":"project"},{"authors":null,"categories":null,"content":"GeoScents is a game that pits online users against each other in a challenge to guess where a given location is in a certain amount of time. It is a remake of an old game called GeoSense, made by a reddit user, u/mattfel. I heard about this game through a r/webgames thread.\nDespite memorizing capitals once in my life, it turns out that I am very bad at the game. After losing a few rounds really badly, I decided to use an afternoon to make a program to beat the game for me. There were 3 things needed:\n Reading the place given to players Looking up where that place is (latitude/longitude coordinates) Moving the mouse to where the place is on the world map  I decided to use Python-tesseract, a wrapper for Google\u0026rsquo;s Tesseract OCR Engine. It can read image files and output the text on the images. To get the image itself, I used something called PyAutoGUI, a library that automates GUI features, to capture a screenshot given coordinates on the screen. To get those coordinates, I needed keyboard input and thus required pynput, a library that controls input devices. When setting up the program, the user has to give coordinates on the screen of where the text of the location to look up will show up. Thus, we can get an image of the text and passing it into pytesseract, we get the location in string.\nTo look up where the place is, I used a libary called geopy. Geopy has OpenStreetMap\u0026rsquo;s Nominatum which allows me to get the latitude and longitude of a place given an address. Thus, all it needed was a simple function to look up the latitude/longitude of a given location. The lookup took less than 1 second in most cases as well.\nFinally, to move the mouse to the given latitude and longitude on the GeoScents map, I used PyAutoGUI. However I ran into a problem - the map given wasn\u0026rsquo;t entirely correct (the equator was much lower than the center of the map). Because this was a hack and not something that needed immense precision (and because GeoScents lets you guess the location within a radius), I decided to forego precision for convenience and instead allowed the user to input 2 known locations and extrapolated where the mouse would be given the scaling. I chose the equator and the Cape of Good Hope as the two places needed to be selected. Once the screen coordinates are received, the program automatically finds the conversion from latitude/longitude coordinates to pixels and thus converts the location coordinates to pixel coordinates on screen.\nAll of the above is wrapped up into a button (in a GUID made with TkInter) which when clicked performs the above 3 steps and automatically moves your cursor to the location on the map. You can adjust for precision based on your knowledge but I just click because I trust the program more than my highschool geography. The program works well when calibrated correctly (correct coordinates for text, equator, and Cape of Good Hope) and achives really high scores! There are a couple of times where the program fails because of smaller text and propagated errors from the calibration. Currently the program only works for the world map. In the future, I hope I\u0026rsquo;ll be able to make it work for all the maps on GeoScents.\n","date":1578441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578441600,"objectID":"ab2fc9ee34732066cf1470ce235b29d6","permalink":"https://amritamar.github.io/project/geoscentshack/","publishdate":"2020-01-08T00:00:00Z","relpermalink":"/project/geoscentshack/","section":"project","summary":"A program I wrote to beat GeoScents.","tags":["Personal","Game Design"],"title":"Hacking GeoScents","type":"project"},{"authors":null,"categories":null,"content":"This is the final project my team and I worked on in graduate computer vision. I give a brief summary of what we did throughout the semester. Unfortunately, I can\u0026rsquo;t release the code as per Cornell regulations, but feel free to read the final paper on the project (linked above). All the following pictures were generated by computers!\nOur goal in this project is to see if it is possible to generate human poses using GANs. We draw inspiration from the paper, Generating Videos with Scene Dynamics, which proposed a Generative Adversarial Network (GAN) for videos that untangle the scene\u0026rsquo;s foreground from the background, also known as VideoGAN. VideoGAN generated videos that, while not photo-realistic, display motions that were fairly reasonable for the scenes it was trained on. Specifically, we explored generating poses by understanding human dancing, by training our models on the Let’s Dance Dataset. We evaluated how well VideoGAN is able to output dance video and we proposed a new architecture called VectorGAN that takes the vector representations of people dancing and generates videos.\nOur intention in this project was to explore how computers can capture motion and generate it. By generating motion from a given frame, we can generate \u0026lsquo;future\u0026rsquo; motion. There are lots of applications for generating “future” video such as video understanding and simulations and animation (specifically in Rigging avatars to automatically create ranges of motion). Given a starting image, we wanted to make predictions about certain events. We used dancing because human motion is fluid and defined in dancing, and the dataset we used was clearly categorized.\nWe implemented the following:\n VideoGAN Architecture - 32 Frames of Video Modified VideoGAN Architecture - 128 Frames of Video VectorGAN Architecture with Random Noise - 32 Frames of Video VectorGAN Architecture with Starting Vector - 32 Frames of Video VectorGAN Architecture Composited Video (using previous experiment) - 128 Frames of Video  VideoGAN: following the inspiration paper, Generating Videos with Scene Dynamics, we implemented their model and generated 32 frames of video. We then modified their architecture and generated 128 frames of video. Overall, from the output video, this model does well in making a figure of a human doing a dance move. However, it is very noisy. We expected this from a model that takes in raw pixels. This also happened in the second experiment where, even though the generated dance video was longer, it was still blurry. However, VideoGAN recognized people and captured some of their movements (blue is 32 frames, green is 128 frames).\nVectorGAN: using the dance video\u0026rsquo;s vector representation, we trained our model on the positions of human parts. Although VectorGAN had to be trained on more epochs to get something smooth, this method worked pretty well. The generated vector positions through time resemble a lot of the motion that ballet dancers do. It is interesting to note that the model figured out how the different limbs of humans connect to each other after about 100 epochs but needed much more to generate coherent movement. There are still a lot of places where the dancer’s limbs jerk, but we attribute this to the shortcomings of the dataset we used; not every single frame had a coherent human structure and some frames had nothing. We then tested the VectorGAN with an encoder added on to the beginning of the architecture that had the starting vectors of the input video. We then used this in the composited video experiment, where we generated 128 frame videos by feeding the last frame of a generated 32 frame video into the model 3 times (random noise, starting vector, and 128 frames in that order below).\nOverall, VideoGAN and VectorGAN provide tangible results in generating videos of human pose. However, the frames generated VideoGAN were of very low resolution as computing images of stick figures is still very difficult. VectorGAN had much better results as we could animate together distinct key points to see the figure move more clearly. Our conclusion is that generating human pose is possible with VectorGAN, but we don’t have nearly enough training data. The original VideoGAN paper used over 35 million clips while we used only 84 videos for training our models. However, the results we got are promising. We only used the ballet dances from the dataset, but it would be interesting (and very time consuming) to see how the model does on other dances.\n","date":1576713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576713600,"objectID":"55590df8f5ee2fb7d405c6f923e8e0dd","permalink":"https://amritamar.github.io/project/dancegans/","publishdate":"2019-12-19T00:00:00Z","relpermalink":"/project/dancegans/","section":"project","summary":"A GANs Project for CS 6670 (Graduate Computer Vision).","tags":["Cornell","AI"],"title":"Everybody GANs Now!","type":"project"},{"authors":null,"categories":null,"content":"This was a project I made after starting to learn javascript using p5.js. I really like the idea of how code can be used to visualize abstract math concepts (and show what exactly is happening in the background) and I hope to continue making more visualizations using code in the future. First a few references: I got interested in fourier transforms after watching 3Blue1Brown\u0026rsquo;s video on the topic. I also saw this really cool and interactive introduction to the topic by @jezzamonn. Finally, I learnt javascript (and most of this project) by watching Daniel Shiffman\u0026rsquo;s Coding Challenges!\nWhat is a fourier transform? It\u0026rsquo;s simply a way of breaking apart a wave into individual components. This is most used in signal processing, specifically in the audio area. One of the coolest things you can do with fourier transforms is extract specific sounds and tune them however you like! So for example, if you had a music clip with a bit of fuzzy noise in the background, you can use a fourier transform on the clip and get the individual sound waves. After finding the one causing the fuzzy noise, you can remove it. Combine the remaining waves and voila - you have a sound clip without that fuzzy sound.\nFollowing Daniel Shiffman\u0026rsquo;s tutorial, I first started with visualizing the fourier series. I visualized the square wave and the sawtooth wave. You can adjust the sliders to increase/reduce the number of epicycles, and use the dropdown menu to change the wave in question.\n\r Signals, or waves, can be interpreted as a set of points. One for the X axis, and one for the Y axis. If we apply the fourier transform to both of them, we can 2 transforms that give us the frequence, phase, and amplitude of each component. This is what I did in the next iteration!\n\r I then, instead of simply drawing a circle from a set of points, implemented the ability to create the signal/drawing yourself! Once you finish drawing, the program does a fourier transform, breaking your drawing into X and Y components. Try it out - click and draw something!\n\r The final part of this project involved combining the 2 parts, X and Y components, into 1 Complex Number component. This allows you to use only one overall circle to draw both parts of the transform as the complex number component captures both X and Y of the drawing. Try the final version out!\nIn addition:\n use A/D to reduce/increase the number of epicycles used\n use W/S to increase/reduce the speed\n  \u0026hellip; and see how it affects the fourier transform!\n\r I love the way code can be used to visualize math concepts. I hope to do more of these in the future, both to understand the math behind tricky concepts and to improve my javascript skills!\n","date":1565654400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565654400,"objectID":"8c4ef68328e99f9a2aaba3b46a73997f","permalink":"https://amritamar.github.io/project/fouriertransforms/","publishdate":"2019-08-13T00:00:00Z","relpermalink":"/project/fouriertransforms/","section":"project","summary":"A fun mini-project I did to understand fourier transforms.","tags":["Personal"],"title":"Exploring and Understanding Fourier Transforms using p5.js","type":"project"},{"authors":null,"categories":null,"content":"This is an old project I recently ported over to JS. The original code for the project was written in Visual Basic 6, the first language I learnt. I returned to this project while learning JS (specifically using p5.js). It\u0026rsquo;s a 2 player game that adds an extra layer of thinking to the original game of tic tac toe.\n\r I hope to continue this by adding AI players!\n","date":1565568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565568000,"objectID":"c32c9cf73a8f11a87e6d290d6fccac82","permalink":"https://amritamar.github.io/project/ultimatetictactoe/","publishdate":"2019-08-12T00:00:00Z","relpermalink":"/project/ultimatetictactoe/","section":"project","summary":"A small game I made to learn more JS.","tags":["Personal","Game Design"],"title":"Ultimate Tic Tac Toe!","type":"project"},{"authors":null,"categories":null,"content":"A star has fallen from the sky and the race is on to go claim it. However, the monsters around the area will not make it easy. Players can choose to play as a warrior, rogue, or wizard to blast through enemies by choosing powerful spells or attacks and casting them as quickly as possible while dodging enemy blows to stay alive.\nStelliform is a real-time action card game that relies on quick swiping mechanics, split-second decision making, and pre-game planning. The integration of a real-time strategy card game with a stimulating swipe mechanic makes Stelliform a distinct and unique mobile game. By combining strategic card-based elements and real-time action, we want players to step into the shoes of a spellcaster, defeating enemies by constructing a spell deck and planning and casting spells under time pressure.\nThe game was produced by Team Sicko Code, consisting of:\n Amrit Amar (Project Lead) Alan Pascual (Architecture Lead) Luke Shin (Programmer) Brandon Zhang (Programmer) Hanna Arfine (UI/UX Artist) Ally Yuan (Character Artist) Michael Yee (Music)  The game is made using CUGL (Cornell University Graphics Library), a game engine designed by Professor Walker White in C++. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented graphics and animations in C++, along with Android Building and testing.\n\r\r\r\r\r\r\r ","date":1558051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558051200,"objectID":"db90a4a922d33ad1d008a9bb887be83f","permalink":"https://amritamar.github.io/project/stelliform/","publishdate":"2019-05-17T00:00:00Z","relpermalink":"/project/stelliform/","section":"project","summary":"An action, strategy mobile game my team made for CS 4152 (Advanced Game Design).","tags":["Game Design","Android","Cornell"],"title":"Stelliform","type":"project"},{"authors":null,"categories":null,"content":"You\u0026rsquo;ve crashed on a mysterious world after getting caught in an explosion. You\u0026rsquo;ve lost the shards that allowed you to fly and explore the galaxy. Make your way through different environments to find the shards and escape from the planet!\nViridian is an action webgame made that relies on precise mechanics to make your way through a magical forest. Use your mouse to shoot and gain momentun as you travel through 3 biomes. Beware of shooting plants, however, as the forest is connected to each other and plants will actively try to defend their fellow flora.\nThe game, featured on the frontpage of Newgrounds, was produced by Team LVI Studios, consisting of:\n Amrit Amar (Project Lead) Alan Pascual (Architecture Lead) Justin Lue (Programmer) Brandon Zhang (Programmer) Yanying (Mary) Ji (UI/UX Artist) Urael Xu (Character Artist)  The game is made using Unity Game Engine. Throughout development, the team released several builds of the game and gathered data using A/B testing on statistics such as player deaths, time spent on levels, and other indicators that the team used to incrementally build a better game. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented the levels and graphics in C#.\n\r\r\r\r\r ","date":1544918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544918400,"objectID":"5f68375a27770bf6689394ca77bb43a1","permalink":"https://amritamar.github.io/project/viridian/","publishdate":"2018-12-16T00:00:00Z","relpermalink":"/project/viridian/","section":"project","summary":"An action web game my team made for CS 4154 (Analytical Game Design).","tags":["Game Design","Web","Cornell"],"title":"Viridian","type":"project"},{"authors":null,"categories":null,"content":"This is a project I did to learn about Neural Networks and Genetic Algorithms. We create a neural network to drive a car and then use genetic algorithms to train and find the best weights for the neural network. The network takes in 6 inputs and outputs the acceleration and steering of the car.\nHere, we see all the cars beginning to get initialized with a random neural network. They are pretty dumb.\nIn the editor, this is the information that cars get (as shown by the yellow lines).\nBy generation 22, we have a car that can finish the whole track (albeit not smoothly)!\nOn average, it takes about 22 generations to train the neural network fully.\nThe neural network has 6 input nodes:\n current speed distance of closest obstacle from the left side (max 10m) distance of closest obstacle from the right side (max 10m) distance of closest obstacle from the front (max 10m) distance of closest obstacle from the front-left side (max 10m) distance of closest obstacle from the front-right side (max 10m)  There is 1 hidden layer, consisting of 5 nodes. There is 1 output layer, consisting of 2 nodes:\n Acceleration (-1 to 1) Steering (-1 to 1)  I use tanH as my activation function.\nThere are 3 main scripts:\n CarController: this is the script for a car. Has a neural network that drives the car. CarManager: manager for the population of cars NeuralNetwork: holds the neural network code GeneticAlgorithmController: performs the selection process of the algorithm The program works as follows:  The input and hidden layer nodes have both a weight and a bias, that is randomly generated for the population of cars (default 10). They are then simulated as generation 1 on the race track. After all the cars crash or 60 seconds, the algorithm chooses the best 2 cars (based on displacement from starting point) and then creates the next generation of cars by combining the best 2 cars genes (randomly selecting between the 2 or a combination thereof). There is an option that allows us to keep the best 2 cars in the next generation or to generate all new cars in the population from the parents (i.e. the parents die). The next generation is then simulated and so on.\n","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544832000,"objectID":"ce49541bf0ac474129da0ae7da631934","permalink":"https://amritamar.github.io/project/geneticselfdrivingcars/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/project/geneticselfdrivingcars/","section":"project","summary":"A project I made to explore neuroevolution.","tags":["Personal","AI"],"title":"Genetic Self-Driving Cars","type":"project"},{"authors":null,"categories":null,"content":"This is a JS implementation of a particle system. The project simulates particles in a \u0026lsquo;box\u0026rsquo; and assigns random forces between different types of particles. Interestingly, certain behaviors arise that are \u0026lsquo;life-like\u0026rsquo; from these rules. The particles interact with each other arising in behaviors that mimic cell behavior.\nThis simulation works by having each particle type have a certain force associated with another particle type. Sometimes the force can attract, sometimes it repels. The force is stronger the closer the two particles are, but after a certain point, they do not affect each other. The randomization of these forces leads to a number of interesting behaviors. You can play with the simulation in the widget below!\nYou can control the number of particles (and click to add more of a certain type), the interaction radius controls the distance at which particles interact with each other (a global variable here), and a few other options. Feel free to edit the code and try out other values in the web code editor.\n\r I was inspired by this from an Evolution Class I took at Cornell where I read the theories of biologist Lynn Margulis. I was further inspired to create my own implementation from Jeffrey Ventrella\u0026rsquo;s Clusters, which has examples in both the browser and in augmented reality. Furthermore, I also drew inspiration from this study and HackerPoet and his Particle-Life Video. The main idea, from Lynn Margulis is that all organisms naturally gravitate to other kinds of organisms, and likewise are repelled by different organisms. These particles experience attractions and repulsions with other particles of different colors. They cluster into social pods, or scatter and flee, often mimicking biological behaviors. Clusters shapeshift and exchange parts of their identities, much like the tiny organisms that came together in early evolution to form symbiotic unions.\n","date":1542240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542240000,"objectID":"973ca48bf103ed0e2771ae8d8309a6b2","permalink":"https://amritamar.github.io/project/particlesimulationoflife/","publishdate":"2018-11-15T00:00:00Z","relpermalink":"/project/particlesimulationoflife/","section":"project","summary":"A project I made to explore the theories by biologist Lynn Margulis.","tags":["Personal","AI"],"title":"A Particle Simulation of Life","type":"project"},{"authors":null,"categories":null,"content":"You are a lab assistant who has accidentally shattered the fabric of reality and created numerous rifts between worlds. Time, gravity, and even color have been drained from the world. Now you must travel through these rifts, recover the scattered shards of space, and restore them to the riftss in order to fix the damage to reality.\nOutOfSync is a action, strategy platformer that relies on quick decision making and mechanics. You must plan your route and be quick as you collect the shards, return back to repair the rifts, and move to the next level. However, as you are doing this, clones of you will take your exact path - collide with them and you lose the game.\nThe game was produced by Team Tektite, consisting of:\n Amrit Amar (Project Lead) Alan Pascual (Architecture Lead) Luke Shin (Programmer) Brandon Zhang (Programmer) Cathy Liu (UI/UX Artist) Sarah Skrutskie (Character Artist)  Special thanks to Adam Pascual for an amazing soundtrack!\nThe game is made using LibGDX and Box2D. As project lead, I managed the team, worked on imagining the core gameplay mechanics, set up meetings and developed a workflow and plan. Deliverables were due every two weeks and I learned how to manage a team effectively. As a programmer, I implemented graphics and animations in Java, along with building and testing the game.\n\r\r\r\r\r ","date":1526428800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526428800,"objectID":"a8b90b2077b67ec17cabfd19cb449a66","permalink":"https://amritamar.github.io/project/outofsync/","publishdate":"2018-05-16T00:00:00Z","relpermalink":"/project/outofsync/","section":"project","summary":"A strategic, action platformer my team made for CS 3152 (Beginners Game Design).","tags":["Game Design","PC","Cornell"],"title":"OutOfSync","type":"project"},{"authors":null,"categories":null,"content":" This is a project I made for my Artificial Intelligence Class under the Team \u0026ldquo;Real Intelligence\u0026rdquo;. It is a image generation program that uses primitive shapes to generate images using genetic algorithms.\nOur intention was to complete a project exploring the concept of genetic algorithms within artificial intelligence. Genetic Algorithms use the concept of natural selection to find a minima to a function. In this case, the genetic algorithm tries to replicate a picture using only primitive shapes and tries to bring the generated picture as close to the real picture as possible.\nTo generate the image, we can adjust the number of shapes, the mutation rate, and the types/combinations of shapes to use in the generated image: - Circles (mutate the position, radius, color) - Ellipses (mutate the position, width, height, color) - Rectangles (mutate the position, width, height, color) - Pixels (n-pixels long squares, mutate the pixel position, color) - n-sided polygons\nOur implementation focuses on using a single-parent. This means that during each generation, only one picture is used to create all the children. A parent is replaced when one of the children has a better fitness score (or worse, rather). Our fitness function work by comparing the child to the original image pixel by pixel.\nThe pseudocode for our genetic algorithm is as follows:\n Generate a random image based on user-defined configurations, mainly number of shapes and types of shapes to use. Every ‘individual’ image has a ‘DNA’ that is a collection of shapes. These shapes are randomly generated and are assigned random colors. This individual image’s fitness score is also calculated here. We then generate a new image with a certain mutation rate. This new image will have random differences from the parent image (differences ranging from changing the shapes’ position or dimensions to changing the shapes’ color). The new image is then compared with the original image. We then compare the new image’s fitness score with the parent’s fitness score. If the image’s fitness score is lower than that of the parent image (i.e, there is less error in this image, thus the new image is closer to the original picture than the parent image), then we update the parent image to be the new image. If the fitness score is higher than the parent’s score, then we simply keep the same parent for the next generation. Go to part 2 and repeat until fitness score of the parent is 0.  Example 1: Eiffel Tower: Target Picture: Starting the Algorithm: After a bit of time: After a long time: Notice how the sky gradient is captured by the generated picture, simply by the adjustment of the alpha of overlapping shapes!\nOver time, the error in the generated images decreases. Of course, this happens over thousands of generations. As we used our program with a multitude of images, we noticed that some shapes converge to target images better than other shapes. For the full evaluation, check out our report.\nExample 2: Mona Lisa: Target Picture: Starting the Algorithm: After a bit of time: After a long time: Surprisingly, the algorithm seems to capture the background really well! Of course, facial features will take quite a while to develop as there is not much of a pixel difference from this image and one with facial features.\n","date":1525910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525910400,"objectID":"e0a1d0dbe530c5e5b230b8501a15d129","permalink":"https://amritamar.github.io/project/geneticimages/","publishdate":"2018-05-10T00:00:00Z","relpermalink":"/project/geneticimages/","section":"project","summary":"A project I made for CS 4701 (Practicium in Artificial Intelligence).","tags":["Cornell","AI"],"title":"Genetic Images","type":"project"},{"authors":null,"categories":null,"content":"This is a hand that can be used to do a lot of things. The basic motive of the project is that the user will wear the control glove, and will be able to control the robotic hand using their movements. A close friend and I made a robotic animatronic hand that could potentially help save lives in the event of an accident in Mines, one of Botswana\u0026rsquo;s main industries.\nThere are many uses for this such as Space Exploration and in the Mining Industry. Instead of using AI, that can\u0026rsquo;t react to all situations in an environment, we can use robots, that are controlled by the user. Humans can react to sudden environmental changes and, using \u0026ldquo;The Hand\u0026rdquo;, they can improvise and adapt.\nFor making the Project, we used an Arduino Leonardo, Flex Sensors and Servo Motors. The Arduino is a microprocessor that basically reads from code uploaded to it from a computer (C/C++ Code) and therefore through the use of Digital and Analog I/O pins, executes the code. Flex Sensors are Variable Resistors that change values depending on how much they are bent by. Servo Motors are precision DC motors that can be controlled to point to the exact angle from 0 to 180 Degrees.\nWhat basically happens here is that the Flex Sensor readings are inputs through the Analog Pins. It is processed and mapped to a angle. The angle is then written to the Servo Motors, thus turning them and therefore moving the fingers. There are also two buttons that control a LED and a Speaker. On the breadboard, there is an LDR (Light dependent resistor) that basically measure the amount of light in a room. Using those values, it gives a green light or a yellow light. The Arduino can be used to do a lot of things!\nIn the future, we hope to develop a better working hand with all fingers working and an opposable thumb. This would allow us to hold things. The next stage of this project is to finish the hand and make sure it can pick up things.\nWe won first place at a National Competition (The Botho College ICT Linkz Challenge 2014) with this project. Thank you to everyone who helped out!\nA Video Demonstration can be found here\n","date":1411257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1411257600,"objectID":"9c6554c5387d14ed6c13af7ba653318c","permalink":"https://amritamar.github.io/project/the-hand/","publishdate":"2014-09-21T00:00:00Z","relpermalink":"/project/the-hand/","section":"project","summary":"An IT Innovation project I made for the Botho College ICT Challenge, a national competition.","tags":["Arduino","Personal"],"title":"The Hand","type":"project"},{"authors":null,"categories":null,"content":"Youcandoit!\r\r\rI believe in you! If you can\u0026rsquo;t believe in yourself, believe in the me who believes in you! You\u0026rsquo;re handsome and you\u0026rsquo;re smart and you\u0026rsquo;re the best.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"01da1065de75311039b9e5a832d8ffab","permalink":"https://amritamar.github.io/moralsupport/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/moralsupport/","section":"","summary":"moralsupport","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://amritamar.github.io/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://amritamar.github.io/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"Experiences","tags":null,"title":"AmritAmar","type":"widget_page"}]